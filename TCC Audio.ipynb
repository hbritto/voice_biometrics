{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registering new person\n",
    "import os\n",
    "from audio_utils.recorder import Recorder\n",
    "\n",
    "base_dir = '/run/media/hbritto/Data/Datasets/deep-speaker-data'\n",
    "# base_dir = '/home/hbritto/code/tcc/deep-speaker-data'\n",
    "audio_dir = base_dir + '/VCTK-Corpus/'\n",
    "cache_dir = base_dir + '/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from deep_speaker.audio_reader import AudioReader\n",
    "from deep_speaker.constants import c\n",
    "with open(os.path.join(cache_dir, 'embeddings.pkl'), 'rb') as pkl:\n",
    "    all_embs = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input('Nome da pessoa a ser registrada: ')\n",
    "n_audio = int(input('Número de áudios a serem gravados: '))\n",
    "person_folder = os.path.join(base_dir, name)\n",
    "os.makedirs(person_folder, exist_ok=True)\n",
    "rec = Recorder()\n",
    "\n",
    "for i in range(n_audio):\n",
    "    print('Áudio número {:>02d} de {:>02d}'.format(i + 1, n_audio))\n",
    "    with rec.open(os.path.join(person_folder, '{}_{:>03d}.wav'.format(name, i))) as recfile:\n",
    "        recfile.record(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_audio_dir = os.path.join(base_dir, person_folder, '')\n",
    "audio_reader = AudioReader(input_audio_dir=audio_dir,\n",
    "                           output_cache_dir=cache_dir,\n",
    "                           sample_rate=c.AUDIO.SAMPLE_RATE,\n",
    "                           multi_threading=True)\n",
    "audio_reader.build_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings will be normalized.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 390)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 200)               78200     \n",
      "_________________________________________________________________\n",
      "normalization (Lambda)       (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embeddings (Lambda)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 105)               21105     \n",
      "=================================================================\n",
      "Total params: 99,305\n",
      "Trainable params: 21,105\n",
      "Non-trainable params: 78,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "********************************************************************************\n",
      "[0.07174713 0.05610642 0.05632469 0.07125094 0.05404102 0.0687568\n",
      " 0.07772089 0.0725681  0.07134654 0.06884344 0.06318593 0.06498828\n",
      " 0.06006791 0.04880178 0.06170165 0.05511497 0.0761371  0.0632759\n",
      " 0.0772002  0.05797705 0.06659994 0.07500102 0.05599819 0.05951253\n",
      " 0.06329308 0.05938116 0.06293306 0.06408758 0.08202746 0.06109606\n",
      " 0.08215938 0.05894056 0.05691764 0.05445581 0.06649138 0.06582313\n",
      " 0.06000488 0.0770919  0.05669566 0.077044   0.05132568 0.06182598\n",
      " 0.06418463 0.06963346 0.05122191 0.0479783  0.07190189 0.05956694\n",
      " 0.08300497 0.05441856 0.07082    0.07313047 0.06262931 0.07618123\n",
      " 0.06375849 0.06278118 0.0684841  0.0647015  0.06485453 0.07393759\n",
      " 0.07406676 0.06325181 0.07073563 0.06107729 0.07347568 0.05437813\n",
      " 0.07872708 0.04729054 0.09057967 0.05123297 0.06295172 0.04992595\n",
      " 0.04650111 0.07405874 0.04720128 0.05372341 0.07908539 0.06373908\n",
      " 0.06490046 0.06825128 0.06658494 0.05332203 0.05285872 0.06313419\n",
      " 0.0607366  0.06018707 0.05543552 0.05772999 0.06888868 0.05890168\n",
      " 0.06233276 0.06928022 0.07001454 0.06470893 0.0635614  0.06613883\n",
      " 0.0659592  0.04536878 0.05505892 0.07503159 0.07387798 0.06925701\n",
      " 0.06856849 0.06542188 0.06186942 0.08668672 0.06186872 0.07052616\n",
      " 0.05049865 0.07162029 0.08917148 0.0452101  0.04220936 0.07210591\n",
      " 0.07167663 0.06436624 0.07623109 0.06662169 0.06452549 0.0593702\n",
      " 0.07118779 0.06036615 0.06976213 0.05496876 0.05470077 0.0672724\n",
      " 0.05830326 0.07163808 0.06506602 0.06340389 0.07064208 0.06964614\n",
      " 0.06969889 0.04855169 0.07474266 0.05812144 0.0702956  0.06800563\n",
      " 0.05912157 0.06938078 0.05154052 0.07003555 0.06019867 0.06963041\n",
      " 0.0579408  0.06807674 0.06280925 0.06150609 0.06880377 0.06905782\n",
      " 0.0655103  0.06363396 0.0673234  0.07445913 0.06817758 0.06903961\n",
      " 0.06628599 0.07424318 0.05706462 0.07127371 0.05796282 0.05944207\n",
      " 0.07336397 0.06965956 0.05200154 0.07333378 0.06508649 0.06235888\n",
      " 0.05906126 0.06395912 0.06447854 0.06408676 0.06283801 0.0878389\n",
      " 0.06612647 0.0718235  0.06320605 0.05160533 0.05849358 0.05031389\n",
      " 0.07304921 0.0615244  0.05699734 0.06322673 0.05840187 0.06574978\n",
      " 0.07041364 0.08112375 0.05567894 0.07538452 0.06618848 0.06720075\n",
      " 0.06806923 0.05934334 0.06984492 0.05358994 0.05590965 0.06621688\n",
      " 0.05871458 0.0610574 ]\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from deep_speaker.unseen_speakers import inference_embeddings\n",
    "emb_teste = inference_embeddings(audio_reader, 'p225')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_embs.update({'Teste': emb_teste})\n",
    "# del all_embs['Teste']\n",
    "print(emb_teste)\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=1, metric='cosine')\n",
    "nn.fit(list(all_embs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.kneighbors(emb_teste.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() takes 3 positional arguments but 111 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ee5e8c9b878d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoice_recogniser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVoiceRecogniser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrecog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVoiceRecogniser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrecog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_embs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/tcc/voice_biometrics/blocks/voice_recogniser.py\u001b[0m in \u001b[0;36mpersons\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_to_person\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/tcc/voice_biometrics/blocks/voice_recogniser.py\u001b[0m in \u001b[0;36m_fit_nn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecognise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() takes 3 positional arguments but 111 were given"
     ]
    }
   ],
   "source": [
    "from blocks.voice_recogniser import VoiceRecogniser\n",
    "recog = VoiceRecogniser()\n",
    "recog.persons = all_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog.recognise(emb_teste)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
